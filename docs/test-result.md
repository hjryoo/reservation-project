# 1. 테스트 수행 개요

본 테스트는 콘서트 예약 시스템의 핵심 기능인 조회, 예약, 결제 프로세스에 대해 k6를 활용한 부하 테스트를 수행하였습니다. 특히 1,000개의 한정된 좌석에 대해 다수의 사용자가 동시에 접근하는 고강도 경쟁(High Contention) 상황을 시뮬레이션하여 시스템의 동시성 제어 능력과 안정성을 검증했습니다.
# 2. 상세 분석 결과
   2.1 종합 성능 지표

| 지표 (Metric)        | 측정값 (Measured)  | 목표치 (Target) | 결과 (Result) | 비고                         |
| ------------------ | --------------- | ------------ | ----------- | -------------------------- |
| System Reliability | 100% (Error 0건) | 99.99%       | PASS        | 5xx 서버 에러가 단 한 건도 발생하지 않음  |
| Throughput         | ~88 TPS         | 50 TPS       | PASS        | 로컬 Docker 환경 기준 준수한 처리량 달성 |
| Latency (p95)      | ~14.6ms         | 500ms        | PASS        | 목표치 대비 30배 이상 빠른 초고속 응답 속도 |
| Data Integrity     | Perfect         | -            | PASS        | 중복 예약(Overbooking) 발생 없음   |

2.2 시나리오별 심층 분석
A. 좌석 예약

    현상: 총 3,800여 건의 예약 시도 중 약 88%가 400/409 (Sold Out) 응답을 받았습니다.

    분석:

        이는 시스템 장애가 아니라, 분산락(Distributed Lock)이 정상 작동하여 선착순 1,000명 외의 요청을 유효하게 방어(Business Rejection) 했음을 의미합니다.

        특히 p95 응답 속도가 14ms 수준으로, 락 대기(Lock Waiting)로 인한 병목 현상 없이 매우 효율적으로 트래픽을 처리했습니다.

        결론: 동시성 제어 로직(Redis + DB Lock)이 매우 견고하게 설계되었습니다.

B. 결제

    현상: 404 Not Found 에러가 일부 관측되었습니다. (로그 참조: status: 404, url: .../payments)

    원인 추정: 테스트 스크립트에서 생성한 임의의 reservationId가 실제 DB에 존재하지 않아 발생한 것으로 보입니다. 이는 시스템 부하가 아닌 테스트 데이터 정합성(Data Seeding) 이슈입니다.

    영향: 시스템 리소스(CPU/Mem)에는 영향을 주지 않았으며, 응답 속도(3ms)는 매우 안정적입니다.

C. 콘서트 조회

    성능: 캐싱 전략(Redis Cache) 덕분에 대규모 조회 트래픽에도 DB 부하 없이 즉각적인 응답을 보장하고 있습니다.

# 3. 적정 배포 스펙 추론

테스트 결과와 애플리케이션의 리소스 사용 패턴을 기반으로 최적의 배포 스펙 추론.
3.1 리소스 사용 패턴 분석

    CPU: 예약 요청 시 순간적인 스파이크(Spike) 가 발생하나, 락 획득 대기 중에는 CPU 사용량이 낮습니다. I/O Bound(Redis/DB 통신) 성격이 강합니다.

    Memory: 동시 접속자 수(VU)가 늘어나도 힙 메모리 사용량은 크게 증가하지 않았습니다. (Stateless한 구조적 이점)

| 구분        | 권장 스펙 (Recommended)             | 설정 근거 (Rationale)                                                               |
| --------- | ------------------------------- | ------------------------------------------------------------------------------- |
| CPU       | Limit: 1.0 Core / Req: 0.5 Core | 현재 88 TPS 처리에 0.5 코어 미만이 소모됨. 여유분을 포함하여 1 Core 할당 시 약 200 TPS까지 처리 가능할 것으로 추산됨. |
| Memory    | Limit: 1Gi / Req: 512Mi         | JVM 기본 오버헤드와 트래픽 버퍼를 고려함. -Xmx512m 설정으로 충분히 운영 가능함.                             |
| Scale-Out | Min: 2 Pods                     | 고가용성(HA) 확보를 위해 최소 2중화 구성 권장. (Redis 분산락 사용으로 수평 확장 가능)                         |

    DB Connection Pool: 현재 HikariCP 설정을 기본값(10)에서 20~30으로 소폭 상향 시 처리량 증대 가능성 있음.

    Redis Connection: Netty 기반의 Redis 클라이언트 설정을 최적화하여 락 획득/해제 지연을 최소화할 수 있음.
